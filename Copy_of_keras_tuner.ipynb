{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vikashkumar09/vikas/blob/main/Copy_of_keras_tuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to the Keras Tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/keras_tuner\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called *hyperparameter tuning* or *hypertuning*.\n",
        "\n",
        "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
        "1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n",
        "2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to perform hypertuning for an image classification application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g83Lwsy-Aq2_"
      },
      "source": [
        "Install and import the Keras Tuner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpMLpbt9jcO6",
        "outputId": "4779ebd8-6c2c-4073-8d69-7c8b998821ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 135 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 42.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_leAIdFKAxAD"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReV_UXOgCZvx"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HljH_ENLEdHa"
      },
      "source": [
        "Load the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHlHs9Wj_PUM",
        "outputId": "8b5feacd-73b9-4410-c0bb-03a810248fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7nzcHP25-JA",
        "outputId": "364167e0-8cee-4984-e193-5d8658ec45e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(label_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAn7gGwL6D5D",
        "outputId": "5c38a770-cfd1-45af-93ed-63f006f2d46d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_train[0]"
      ],
      "metadata": {
        "id": "RGQ-ZOQRn7_K",
        "outputId": "43e024ef-ac07-490a-ff26-d39f80f86b3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(img_train[3],cmap='binary')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "S_it8UsY6Z3R",
        "outputId": "c0d75760-30ca-4080-9be0-18d00f9756c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "id": "LuqokKhYoY4t",
        "outputId": "7c4234f1-d291-4d9e-ac29-62aacd7bec90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLVhXs3xrUD0"
      },
      "outputs": [],
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "id": "R-nQafbAtISJ",
        "outputId": "23cbc37c-8883-4df8-c65d-ba097338823f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        6.03086363e-08, 0.00000000e+00, 0.00000000e+00, 7.84012173e-07,\n",
              "        4.40253007e-06, 0.00000000e+00, 0.00000000e+00, 6.03086363e-08,\n",
              "        2.41234545e-07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 6.03086363e-08, 6.03086363e-08, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.80925895e-07, 0.00000000e+00, 2.17111074e-06, 8.20197420e-06,\n",
              "        7.65919594e-06, 3.73913508e-06, 3.25666610e-06, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 6.03086363e-08, 1.80925895e-07,\n",
              "        2.41234545e-07, 0.00000000e+00, 0.00000000e+00, 1.80925895e-07],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.61851789e-07, 0.00000000e+00, 6.15148019e-06, 1.23029604e-05,\n",
              "        1.06143189e-05, 8.08135701e-06, 8.68444295e-06, 7.41796157e-06,\n",
              "        1.38709845e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 7.23703579e-07, 6.03086335e-07, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 9.34783839e-06, 1.42328372e-05,\n",
              "        1.24838862e-05, 1.07349360e-05, 6.45302362e-06, 9.40814698e-06,\n",
              "        9.70968995e-06, 6.57364080e-06, 3.85975272e-06, 1.38709845e-06,\n",
              "        4.64376490e-06, 7.84012263e-06, 4.34222147e-06, 9.04629474e-07],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.03086363e-08,\n",
              "        0.00000000e+00, 4.16129569e-06, 1.24838862e-05, 1.34488246e-05,\n",
              "        1.31472816e-05, 1.30266644e-05, 1.30266644e-05, 9.83030714e-06,\n",
              "        7.65919594e-06, 7.29734438e-06, 7.35765298e-06, 8.80506013e-06,\n",
              "        8.50351717e-06, 5.30715943e-06, 1.03730845e-05, 3.98036991e-06],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 6.03086363e-08, 6.03086363e-08, 6.03086363e-08,\n",
              "        0.00000000e+00, 1.20617260e-05, 1.39916019e-05, 1.39916019e-05,\n",
              "        1.40519105e-05, 1.38106761e-05, 1.34488246e-05, 1.34488246e-05,\n",
              "        1.29663558e-05, 1.28457386e-05, 9.89061573e-06, 7.65919594e-06,\n",
              "        7.41796157e-06, 1.18204916e-05, 1.38106761e-05, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 1.10364790e-05, 1.35694418e-05, 1.30266644e-05,\n",
              "        1.34488246e-05, 1.37503675e-05, 1.41725286e-05, 1.36900589e-05,\n",
              "        1.35091332e-05, 1.33885160e-05, 1.35091332e-05, 1.33282074e-05,\n",
              "        1.34488246e-05, 1.47756145e-05, 1.04333931e-05, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 1.16395659e-05, 1.37503675e-05, 1.31472816e-05,\n",
              "        1.28457386e-05, 1.19411088e-05, 1.08555532e-05, 1.27854291e-05,\n",
              "        1.26648119e-05, 1.27251205e-05, 1.28457386e-05, 1.34488246e-05,\n",
              "        1.32678988e-05, 1.46549974e-05, 1.21823432e-05, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 6.03086363e-08, 1.80925895e-07, 0.00000000e+00,\n",
              "        7.23703579e-07, 1.32075902e-05, 1.32678988e-05, 1.27854291e-05,\n",
              "        1.31472816e-05, 1.15792573e-05, 1.01921587e-05, 1.36900589e-05,\n",
              "        1.25441948e-05, 1.31472816e-05, 1.35091332e-05, 1.27854291e-05,\n",
              "        1.36297504e-05, 1.18808002e-05, 1.26045034e-05, 3.13604869e-06],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 3.61851789e-07, 0.00000000e+00,\n",
              "        5.97055441e-06, 1.47153060e-05, 1.33885160e-05, 1.32678988e-05,\n",
              "        1.31472816e-05, 1.22426518e-05, 1.19411088e-05, 1.33282074e-05,\n",
              "        1.29663558e-05, 1.28457386e-05, 1.33885160e-05, 1.32678988e-05,\n",
              "        1.47756145e-05, 7.17672719e-06, 1.00715415e-05, 3.37728329e-06],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 2.41234545e-07, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.31697470e-06, 1.42328372e-05, 1.37503675e-05, 1.38709847e-05,\n",
              "        1.37503675e-05, 1.44740716e-05, 1.39916019e-05, 1.28457386e-05,\n",
              "        1.31472816e-05, 1.34488246e-05, 1.41122200e-05, 1.30869730e-05,\n",
              "        1.30869730e-05, 1.26045034e-05, 5.54839380e-06, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 6.03086363e-08, 2.41234545e-07,\n",
              "        3.61851789e-07, 4.22160412e-07, 1.20617273e-07, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.42931458e-05, 1.36297504e-05, 1.30869730e-05, 1.34488246e-05,\n",
              "        1.33885160e-05, 1.32075902e-05, 1.33885160e-05, 1.33282074e-05,\n",
              "        1.30266644e-05, 1.34488246e-05, 1.38106761e-05, 1.29663558e-05,\n",
              "        1.31472816e-05, 1.53787023e-05, 4.64376490e-06, 0.00000000e+00],\n",
              "       [0.00000000e+00, 1.80925895e-07, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 3.73913508e-06, 8.74475154e-06, 1.23029604e-05,\n",
              "        1.37503675e-05, 1.24838862e-05, 1.28457386e-05, 1.33282074e-05,\n",
              "        1.31472816e-05, 1.25441948e-05, 1.27251205e-05, 1.31472816e-05,\n",
              "        1.35091332e-05, 1.34488246e-05, 1.32075902e-05, 1.29663558e-05,\n",
              "        1.35091332e-05, 1.47153060e-05, 9.58907276e-06, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        1.08555537e-06, 2.65357971e-06, 4.94530786e-06, 6.45302362e-06,\n",
              "        1.13983306e-05, 1.37503675e-05, 1.32678988e-05, 1.33885160e-05,\n",
              "        1.30869730e-05, 1.36297504e-05, 1.20617260e-05, 1.23632690e-05,\n",
              "        1.27251205e-05, 1.38709847e-05, 1.35091332e-05, 1.41122200e-05,\n",
              "        1.06143189e-05, 1.13380220e-05, 1.50771575e-05, 1.49565403e-05,\n",
              "        1.40519105e-05, 1.43534544e-05, 1.29663558e-05, 0.00000000e+00],\n",
              "       [0.00000000e+00, 3.43759189e-06, 1.12777134e-05, 1.25441948e-05,\n",
              "        1.35091332e-05, 1.33282074e-05, 1.35091332e-05, 1.25441948e-05,\n",
              "        1.23029604e-05, 1.29060472e-05, 1.25441948e-05, 1.26045034e-05,\n",
              "        1.20617260e-05, 9.58907276e-06, 1.47756145e-05, 1.16395659e-05,\n",
              "        1.24235776e-05, 1.34488246e-05, 1.53787023e-05, 1.53787023e-05,\n",
              "        1.33282074e-05, 1.41122200e-05, 1.33282074e-05, 1.27251205e-05,\n",
              "        1.32678988e-05, 1.39916019e-05, 1.48359231e-05, 0.00000000e+00],\n",
              "       [1.80925895e-07, 1.21823432e-05, 1.37503675e-05, 1.35091332e-05,\n",
              "        1.33282074e-05, 1.27251205e-05, 1.27251205e-05, 1.29060472e-05,\n",
              "        1.23632690e-05, 1.23632690e-05, 1.23632690e-05, 1.32678988e-05,\n",
              "        1.44740716e-05, 4.82469068e-06, 9.04629542e-06, 1.53787023e-05,\n",
              "        1.38106761e-05, 1.33282074e-05, 1.13380220e-05, 9.28752979e-06,\n",
              "        1.15189487e-05, 1.26648119e-05, 1.23029604e-05, 1.26045034e-05,\n",
              "        1.33885160e-05, 1.37503675e-05, 1.35694418e-05, 0.00000000e+00],\n",
              "       [5.91024582e-06, 1.40519105e-05, 1.19411088e-05, 1.26648119e-05,\n",
              "        1.33885160e-05, 1.38106761e-05, 1.38106761e-05, 1.41122200e-05,\n",
              "        1.50168489e-05, 1.32678988e-05, 1.16998744e-05, 1.29663558e-05,\n",
              "        1.30869730e-05, 1.45343802e-05, 3.92006132e-06, 4.40253007e-06,\n",
              "        6.39271457e-06, 7.05611001e-06, 1.01318501e-05, 1.32075902e-05,\n",
              "        1.33282074e-05, 1.29663558e-05, 1.30869730e-05, 1.34488246e-05,\n",
              "        1.34488246e-05, 1.35091332e-05, 1.38106761e-05, 1.74895024e-06],\n",
              "       [4.52314771e-06, 1.23029604e-05, 1.27854291e-05, 1.23029604e-05,\n",
              "        1.16395659e-05, 1.23632690e-05, 1.27251205e-05, 1.35694418e-05,\n",
              "        1.30266644e-05, 1.11570962e-05, 1.18808002e-05, 1.24235776e-05,\n",
              "        1.19411088e-05, 1.28457386e-05, 1.44740716e-05, 1.17601830e-05,\n",
              "        1.36900589e-05, 1.47756145e-05, 1.44137630e-05, 1.34488246e-05,\n",
              "        1.31472816e-05, 1.27854291e-05, 1.26045034e-05, 1.33885160e-05,\n",
              "        1.32678988e-05, 1.33282074e-05, 1.38709847e-05, 4.04067850e-06],\n",
              "       [2.89481432e-06, 1.22426518e-05, 1.10364790e-05, 1.16998744e-05,\n",
              "        1.28457386e-05, 1.18808002e-05, 1.11570962e-05, 1.14586392e-05,\n",
              "        1.16998744e-05, 1.15792573e-05, 1.21823432e-05, 1.29060472e-05,\n",
              "        1.32075902e-05, 1.33282074e-05, 1.32678988e-05, 1.42328372e-05,\n",
              "        1.35694418e-05, 1.30266644e-05, 1.20014174e-05, 1.24235776e-05,\n",
              "        1.12174048e-05, 1.09158618e-05, 1.06746274e-05, 1.03730845e-05,\n",
              "        1.09158618e-05, 1.23632690e-05, 1.24235776e-05, 6.93549237e-06],\n",
              "       [0.00000000e+00, 7.35765298e-06, 1.32075902e-05, 1.16395659e-05,\n",
              "        1.07952446e-05, 1.03127759e-05, 1.10364790e-05, 1.18204916e-05,\n",
              "        1.23029604e-05, 1.26648119e-05, 1.28457386e-05, 1.24838862e-05,\n",
              "        1.27251205e-05, 1.26648119e-05, 1.20617260e-05, 1.18204916e-05,\n",
              "        1.16998744e-05, 1.15189487e-05, 1.17601830e-05, 1.15189487e-05,\n",
              "        1.19411088e-05, 1.15792573e-05, 1.06143189e-05, 9.40814698e-06,\n",
              "        1.00715415e-05, 1.06746274e-05, 1.26648119e-05, 5.54839380e-06],\n",
              "       [0.00000000e+00, 0.00000000e+00, 4.46283866e-06, 1.13983306e-05,\n",
              "        1.27854291e-05, 1.15189487e-05, 1.05540103e-05, 1.03730845e-05,\n",
              "        1.05540103e-05, 1.09158618e-05, 1.11570962e-05, 1.13380220e-05,\n",
              "        1.13983306e-05, 1.13380220e-05, 1.16395659e-05, 1.19411088e-05,\n",
              "        1.23029604e-05, 1.26045034e-05, 1.26648119e-05, 1.26648119e-05,\n",
              "        1.27251205e-05, 1.13380220e-05, 1.13380220e-05, 1.16998744e-05,\n",
              "        1.15792573e-05, 1.30266644e-05, 1.02524673e-05, 0.00000000e+00],\n",
              "       [1.20617273e-07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        3.98036991e-06, 1.20617260e-05, 1.33885160e-05, 1.42931458e-05,\n",
              "        1.44137630e-05, 1.45946888e-05, 1.48359231e-05, 1.46549974e-05,\n",
              "        1.47153060e-05, 1.33282074e-05, 1.32678988e-05, 1.16395659e-05,\n",
              "        1.15189487e-05, 1.07952446e-05, 1.09761704e-05, 1.09761704e-05,\n",
              "        1.09158618e-05, 1.06143189e-05, 1.00112329e-05, 1.01318501e-05,\n",
              "        5.97055441e-06, 3.49790048e-06, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.41234534e-06,\n",
              "        3.67882649e-06, 2.65357971e-06, 4.34222147e-06, 2.47265393e-06,\n",
              "        2.11080214e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YEL2H2Ax3e"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a *hypermodel*.\n",
        "\n",
        "You can define a hypermodel through two approaches:\n",
        "\n",
        "* By using a model builder function\n",
        "* By subclassing the `HyperModel` class of the Keras Tuner API\n",
        "\n",
        "You can also use two pre-defined [HyperModel](https://keras.io/api/keras_tuner/hypermodels/) classes - [HyperXception](https://keras.io/api/keras_tuner/hypermodels/hyper_xception/) and [HyperResNet](https://keras.io/api/keras_tuner/hypermodels/hyper_resnet/) for computer vision applications.\n",
        "\n",
        "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQKodC-jtsva"
      },
      "outputs": [],
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  #input layer\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))#output layer\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J1VYw4q3x0b"
      },
      "source": [
        "## Instantiate the tuner and perform hypertuning\n",
        "\n",
        "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, you use the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) tuner.\n",
        "\n",
        "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oichQFly6Y46"
      },
      "outputs": [],
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIhhdKf9VtI"
      },
      "source": [
        "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwhBdXx0Ekj8"
      },
      "source": [
        "Create a callback to stop training early after reaching a certain value for the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT9IkS9NEjLc"
      },
      "outputs": [],
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKghEo15Tduy"
      },
      "source": [
        "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSBQcTHF9cKt",
        "outputId": "e055edef-5d28-4755-8a81-cefb03ecb75c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 39s]\n",
            "val_accuracy: 0.8426666855812073\n",
            "\n",
            "Best val_accuracy So Far: 0.8600000143051147\n",
            "Total elapsed time: 00h 08m 50s\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
            "layer is 512 and the optimal learning rate for the optimizer\n",
            "is 0.0001.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lak_ylf88xBv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McO82AXOuxXh",
        "outputId": "dfbba436-db22-4810-b46c-1335bbac35a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 9.2986 - accuracy: 0.7619 - val_loss: 5.2538 - val_accuracy: 0.8158\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 4.8076 - accuracy: 0.8148 - val_loss: 4.3983 - val_accuracy: 0.8148\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 3.3818 - accuracy: 0.8317 - val_loss: 3.1247 - val_accuracy: 0.8409\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 2.6539 - accuracy: 0.8422 - val_loss: 3.4443 - val_accuracy: 0.8174\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 2.0880 - accuracy: 0.8512 - val_loss: 2.7386 - val_accuracy: 0.8318\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.7260 - accuracy: 0.8576 - val_loss: 2.4842 - val_accuracy: 0.8363\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.4372 - accuracy: 0.8631 - val_loss: 2.6730 - val_accuracy: 0.8073\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.1635 - accuracy: 0.8708 - val_loss: 1.6690 - val_accuracy: 0.8539\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.9967 - accuracy: 0.8752 - val_loss: 1.6900 - val_accuracy: 0.8445\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.8539 - accuracy: 0.8782 - val_loss: 1.4148 - val_accuracy: 0.8452\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.7130 - accuracy: 0.8867 - val_loss: 1.3598 - val_accuracy: 0.8337\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.6035 - accuracy: 0.8897 - val_loss: 1.1994 - val_accuracy: 0.8510\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.5375 - accuracy: 0.8942 - val_loss: 1.3462 - val_accuracy: 0.8267\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4562 - accuracy: 0.8988 - val_loss: 1.0277 - val_accuracy: 0.8612\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3941 - accuracy: 0.9015 - val_loss: 0.8945 - val_accuracy: 0.8570\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3497 - accuracy: 0.9047 - val_loss: 0.8672 - val_accuracy: 0.8622\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3107 - accuracy: 0.9092 - val_loss: 0.8124 - val_accuracy: 0.8622\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2877 - accuracy: 0.9134 - val_loss: 0.7838 - val_accuracy: 0.8637\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2689 - accuracy: 0.9143 - val_loss: 0.7996 - val_accuracy: 0.8637\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2517 - accuracy: 0.9181 - val_loss: 0.8208 - val_accuracy: 0.8648\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2425 - accuracy: 0.9202 - val_loss: 0.7241 - val_accuracy: 0.8639\n",
            "Epoch 22/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2330 - accuracy: 0.9226 - val_loss: 0.6886 - val_accuracy: 0.8712\n",
            "Epoch 23/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2181 - accuracy: 0.9250 - val_loss: 0.7125 - val_accuracy: 0.8702\n",
            "Epoch 24/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2125 - accuracy: 0.9284 - val_loss: 0.7360 - val_accuracy: 0.8702\n",
            "Epoch 25/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2048 - accuracy: 0.9309 - val_loss: 0.7088 - val_accuracy: 0.8784\n",
            "Epoch 26/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2061 - accuracy: 0.9301 - val_loss: 0.6874 - val_accuracy: 0.8763\n",
            "Epoch 27/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1916 - accuracy: 0.9335 - val_loss: 0.7737 - val_accuracy: 0.8672\n",
            "Epoch 28/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1943 - accuracy: 0.9349 - val_loss: 0.6536 - val_accuracy: 0.8783\n",
            "Epoch 29/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1761 - accuracy: 0.9369 - val_loss: 0.6380 - val_accuracy: 0.8748\n",
            "Epoch 30/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1800 - accuracy: 0.9360 - val_loss: 0.7076 - val_accuracy: 0.8735\n",
            "Epoch 31/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1738 - accuracy: 0.9393 - val_loss: 0.7377 - val_accuracy: 0.8753\n",
            "Epoch 32/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1689 - accuracy: 0.9404 - val_loss: 0.7741 - val_accuracy: 0.8785\n",
            "Epoch 33/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1658 - accuracy: 0.9419 - val_loss: 0.7173 - val_accuracy: 0.8702\n",
            "Epoch 34/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1645 - accuracy: 0.9420 - val_loss: 0.8465 - val_accuracy: 0.8700\n",
            "Epoch 35/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1594 - accuracy: 0.9455 - val_loss: 0.7433 - val_accuracy: 0.8795\n",
            "Epoch 36/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1575 - accuracy: 0.9433 - val_loss: 0.7142 - val_accuracy: 0.8756\n",
            "Epoch 37/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1494 - accuracy: 0.9476 - val_loss: 0.6942 - val_accuracy: 0.8791\n",
            "Epoch 38/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1427 - accuracy: 0.9490 - val_loss: 0.6782 - val_accuracy: 0.8777\n",
            "Epoch 39/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1426 - accuracy: 0.9479 - val_loss: 0.7153 - val_accuracy: 0.8794\n",
            "Epoch 40/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1460 - accuracy: 0.9480 - val_loss: 0.6961 - val_accuracy: 0.8809\n",
            "Epoch 41/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1428 - accuracy: 0.9505 - val_loss: 0.7302 - val_accuracy: 0.8731\n",
            "Epoch 42/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1360 - accuracy: 0.9505 - val_loss: 0.7553 - val_accuracy: 0.8797\n",
            "Epoch 43/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1363 - accuracy: 0.9521 - val_loss: 0.7377 - val_accuracy: 0.8818\n",
            "Epoch 44/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1292 - accuracy: 0.9532 - val_loss: 0.7214 - val_accuracy: 0.8794\n",
            "Epoch 45/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1386 - accuracy: 0.9516 - val_loss: 0.7927 - val_accuracy: 0.8754\n",
            "Epoch 46/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1258 - accuracy: 0.9552 - val_loss: 0.7392 - val_accuracy: 0.8819\n",
            "Epoch 47/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1315 - accuracy: 0.9540 - val_loss: 0.7271 - val_accuracy: 0.8838\n",
            "Epoch 48/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1162 - accuracy: 0.9575 - val_loss: 0.7488 - val_accuracy: 0.8847\n",
            "Epoch 49/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1262 - accuracy: 0.9563 - val_loss: 0.7308 - val_accuracy: 0.8832\n",
            "Epoch 50/50\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1124 - accuracy: 0.9594 - val_loss: 0.7661 - val_accuracy: 0.8769\n",
            "Best epoch: 48\n"
          ]
        }
      ],
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate"
      ],
      "metadata": {
        "id": "VXTKS2NL1MyV",
        "outputId": "c802c110-7e3a-48cc-d17e-002df8495802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Model.evaluate of <keras.engine.sequential.Sequential object at 0x7fe8a006db90>>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOTSirSTI3Gp"
      },
      "source": [
        "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoiPUEHmMhCe",
        "outputId": "0a30207a-94e2-42d9-c81a-36861ea7f5de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 8.7342 - accuracy: 0.7649 - val_loss: 5.0889 - val_accuracy: 0.8018\n",
            "Epoch 2/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 4.3225 - accuracy: 0.8153 - val_loss: 3.8027 - val_accuracy: 0.8185\n",
            "Epoch 3/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 3.1919 - accuracy: 0.8301 - val_loss: 3.3323 - val_accuracy: 0.8198\n",
            "Epoch 4/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 2.5112 - accuracy: 0.8411 - val_loss: 4.3902 - val_accuracy: 0.7653\n",
            "Epoch 5/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.9698 - accuracy: 0.8491 - val_loss: 2.6778 - val_accuracy: 0.8259\n",
            "Epoch 6/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 1.5684 - accuracy: 0.8603 - val_loss: 1.9536 - val_accuracy: 0.8445\n",
            "Epoch 7/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 1.3093 - accuracy: 0.8660 - val_loss: 1.9905 - val_accuracy: 0.8237\n",
            "Epoch 8/48\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 1.1147 - accuracy: 0.8699 - val_loss: 1.6179 - val_accuracy: 0.8512\n",
            "Epoch 9/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.9411 - accuracy: 0.8771 - val_loss: 1.5325 - val_accuracy: 0.8470\n",
            "Epoch 10/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.8193 - accuracy: 0.8806 - val_loss: 1.3990 - val_accuracy: 0.8586\n",
            "Epoch 11/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.6758 - accuracy: 0.8881 - val_loss: 1.1834 - val_accuracy: 0.8592\n",
            "Epoch 12/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.6142 - accuracy: 0.8897 - val_loss: 1.1977 - val_accuracy: 0.8531\n",
            "Epoch 13/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.5170 - accuracy: 0.8950 - val_loss: 1.2195 - val_accuracy: 0.8506\n",
            "Epoch 14/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.4864 - accuracy: 0.8972 - val_loss: 1.0713 - val_accuracy: 0.8406\n",
            "Epoch 15/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.4118 - accuracy: 0.9016 - val_loss: 1.1247 - val_accuracy: 0.8558\n",
            "Epoch 16/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3721 - accuracy: 0.9052 - val_loss: 0.9724 - val_accuracy: 0.8527\n",
            "Epoch 17/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.3467 - accuracy: 0.9089 - val_loss: 0.8894 - val_accuracy: 0.8628\n",
            "Epoch 18/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.3010 - accuracy: 0.9131 - val_loss: 0.8772 - val_accuracy: 0.8664\n",
            "Epoch 19/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2857 - accuracy: 0.9144 - val_loss: 0.7974 - val_accuracy: 0.8625\n",
            "Epoch 20/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2555 - accuracy: 0.9182 - val_loss: 0.7842 - val_accuracy: 0.8653\n",
            "Epoch 21/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2443 - accuracy: 0.9231 - val_loss: 0.7011 - val_accuracy: 0.8740\n",
            "Epoch 22/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2368 - accuracy: 0.9231 - val_loss: 0.8072 - val_accuracy: 0.8614\n",
            "Epoch 23/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2280 - accuracy: 0.9239 - val_loss: 0.7733 - val_accuracy: 0.8673\n",
            "Epoch 24/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.2173 - accuracy: 0.9277 - val_loss: 0.7769 - val_accuracy: 0.8611\n",
            "Epoch 25/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2179 - accuracy: 0.9289 - val_loss: 0.6882 - val_accuracy: 0.8749\n",
            "Epoch 26/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2041 - accuracy: 0.9311 - val_loss: 0.7427 - val_accuracy: 0.8659\n",
            "Epoch 27/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1982 - accuracy: 0.9317 - val_loss: 0.7109 - val_accuracy: 0.8747\n",
            "Epoch 28/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1817 - accuracy: 0.9348 - val_loss: 0.7124 - val_accuracy: 0.8710\n",
            "Epoch 29/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1897 - accuracy: 0.9345 - val_loss: 0.7868 - val_accuracy: 0.8661\n",
            "Epoch 30/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1746 - accuracy: 0.9385 - val_loss: 0.7783 - val_accuracy: 0.8665\n",
            "Epoch 31/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1762 - accuracy: 0.9396 - val_loss: 0.7667 - val_accuracy: 0.8711\n",
            "Epoch 32/48\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1761 - accuracy: 0.9389 - val_loss: 0.8026 - val_accuracy: 0.8669\n",
            "Epoch 33/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1624 - accuracy: 0.9424 - val_loss: 0.6996 - val_accuracy: 0.8742\n",
            "Epoch 34/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1593 - accuracy: 0.9431 - val_loss: 0.7043 - val_accuracy: 0.8710\n",
            "Epoch 35/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1668 - accuracy: 0.9422 - val_loss: 0.7106 - val_accuracy: 0.8754\n",
            "Epoch 36/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1574 - accuracy: 0.9456 - val_loss: 0.7141 - val_accuracy: 0.8727\n",
            "Epoch 37/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1527 - accuracy: 0.9477 - val_loss: 0.7029 - val_accuracy: 0.8750\n",
            "Epoch 38/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1520 - accuracy: 0.9458 - val_loss: 0.6739 - val_accuracy: 0.8819\n",
            "Epoch 39/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1478 - accuracy: 0.9470 - val_loss: 0.7324 - val_accuracy: 0.8772\n",
            "Epoch 40/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1354 - accuracy: 0.9502 - val_loss: 0.7401 - val_accuracy: 0.8758\n",
            "Epoch 41/48\n",
            "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1471 - accuracy: 0.9484 - val_loss: 0.7545 - val_accuracy: 0.8725\n",
            "Epoch 42/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1413 - accuracy: 0.9503 - val_loss: 0.7108 - val_accuracy: 0.8777\n",
            "Epoch 43/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1322 - accuracy: 0.9528 - val_loss: 0.7654 - val_accuracy: 0.8818\n",
            "Epoch 44/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1292 - accuracy: 0.9531 - val_loss: 0.7346 - val_accuracy: 0.8812\n",
            "Epoch 45/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1278 - accuracy: 0.9541 - val_loss: 0.7077 - val_accuracy: 0.8831\n",
            "Epoch 46/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1352 - accuracy: 0.9532 - val_loss: 0.7878 - val_accuracy: 0.8768\n",
            "Epoch 47/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1272 - accuracy: 0.9561 - val_loss: 0.7302 - val_accuracy: 0.8798\n",
            "Epoch 48/48\n",
            "1500/1500 [==============================] - 4s 2ms/step - loss: 0.1242 - accuracy: 0.9555 - val_loss: 0.7584 - val_accuracy: 0.8832\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe83a4b8810>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqU5ZVAaag2v"
      },
      "source": [
        "To finish this tutorial, evaluate the hypermodel on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E0BTp9Ealjb",
        "outputId": "615fcd41-9967-4e6e-a6d4-9970dd9f300e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.8360 - accuracy: 0.8773\n",
            "[test loss, test accuracy]: [0.8360124230384827, 0.8773000240325928]\n"
          ]
        }
      ],
      "source": [
        "eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQRpPHZsz-eC"
      },
      "source": [
        "The `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKwLOzKpFGAj"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial, you learned how to use the Keras Tuner to tune hyperparameters for a model. To learn more about the Keras Tuner, check out these additional resources:\n",
        "\n",
        "* [Keras Tuner on the TensorFlow blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
        "* [Keras Tuner website](https://keras-team.github.io/keras-tuner/)\n",
        "\n",
        "Also check out the [HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) in TensorBoard to interactively tune your model hyperparameters."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}